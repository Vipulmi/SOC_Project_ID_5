{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gymnasium\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install gymnasium\\[box2D\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!brew install swig\n",
    "!pip install \"gymnasium[box2D]\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value_function, rewards\n\u001b[1;32m     46\u001b[0m env_cont \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLunarLander-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m, render_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 47\u001b[0m value_function, rewards_cont \u001b[38;5;241m=\u001b[39m linear_value_function_approximation(env_cont)\n",
      "Cell \u001b[0;32mIn[6], line 29\u001b[0m, in \u001b[0;36mlinear_value_function_approximation\u001b[0;34m(env, num_episodes, gamma, learning_rate)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m     28\u001b[0m     action \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample()\n\u001b[0;32m---> 29\u001b[0m     next_state, reward, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     30\u001b[0m     done \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n\u001b[1;32m     31\u001b[0m     total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/gymnasium/wrappers/time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     47\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/gymnasium/wrappers/order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/gymnasium/wrappers/env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/gymnasium/envs/box2d/lunar_lander.py:675\u001b[0m, in \u001b[0;36mLunarLander.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    672\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[1;32m    674\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 675\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender()\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(state, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32), reward, terminated, \u001b[38;5;28;01mFalse\u001b[39;00m, {}\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/gymnasium/envs/box2d/lunar_lander.py:787\u001b[0m, in \u001b[0;36mLunarLander.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    785\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscreen\u001b[38;5;241m.\u001b[39mblit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msurf, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m    786\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[0;32m--> 787\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclock\u001b[38;5;241m.\u001b[39mtick(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrender_fps\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    788\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mflip()\n\u001b[1;32m    789\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "class LinearValueFunction:\n",
    "    def __init__(self, state_dim, learning_rate=0.01):\n",
    "        self.weights = np.zeros(state_dim)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def predict(self, state):\n",
    "        return np.dot(self.weights, state)\n",
    "\n",
    "    def update(self, state, target):\n",
    "        prediction = self.predict(state)\n",
    "        error = target - prediction\n",
    "        self.weights += self.learning_rate * error * state\n",
    "\n",
    "def linear_value_function_approximation(env, num_episodes=50000, gamma=0.9, learning_rate=0.01):\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    value_function = LinearValueFunction(state_dim, learning_rate)\n",
    "    rewards = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            total_reward += reward\n",
    "\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target += gamma * value_function.predict(next_state)\n",
    "\n",
    "            value_function.update(state, target)\n",
    "            state = next_state\n",
    "\n",
    "        rewards.append(total_reward)\n",
    "        if (episode + 1) % 1000 == 0:\n",
    "            print(f'Episode {episode + 1}, Total Reward: {total_reward}')\n",
    "\n",
    "    return value_function, rewards\n",
    "\n",
    "env_cont = gym.make(\"LunarLander-v2\", render_mode = \"human\")\n",
    "value_function, rewards_cont = linear_value_function_approximation(env_cont)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_bins(num_bins, lower_bound, upper_bound):\n",
    "#     return np.linspace(lower_bound, upper_bound, num_bins + 1)[1:-1]\n",
    "\n",
    "# def digitize_state(state, bins):\n",
    "#     digitized_state = []\n",
    "#     for i in range(len(state)):\n",
    "#         digitized_state.append(np.digitize(state[i], bins[i]))\n",
    "#     return tuple(digitized_state)\n",
    "\n",
    "# class QLearningAgent:\n",
    "#     def __init__(self, state_bins, action_space, learning_rate=0.1, discount_factor=0.99, exploration_rate=1.0):\n",
    "#         self.state_bins = state_bins\n",
    "#         self.action_space = action_space\n",
    "#         self.learning_rate = learning_rate\n",
    "#         self.discount_factor = discount_factor\n",
    "#         self.exploration_rate = exploration_rate\n",
    "#         # self.exploration_decay = exploration_decay\n",
    "#         # self.min_exploration_rate = min_exploration_rate\n",
    "#         self.q_table = np.zeros(state_bins + (action_space.n,))\n",
    "\n",
    "#     def choose_action(self, state):\n",
    "#         if np.random.rand() < self.exploration_rate:\n",
    "#             return self.action_space.sample()\n",
    "#         return np.argmax(self.q_table[state])\n",
    "\n",
    "#     def update_q_value(self, state, action, reward, next_state):\n",
    "#         best_next_action = np.argmax(self.q_table[next_state])\n",
    "#         td_target = reward + self.discount_factor * self.q_table[next_state][best_next_action]\n",
    "#         td_error = td_target - self.q_table[state][action]\n",
    "#         self.q_table[state][action] += self.learning_rate * td_error\n",
    "\n",
    "#     # def decay_exploration_rate(self):\n",
    "#     #     if self.exploration_rate > self.min_exploration_rate:\n",
    "#     #         self.exploration_rate *= self.exploration_decay\n",
    "\n",
    "# def q_learning(env, num_episodes=50000, num_bins = 24, learning_rate=0.1, gamma=0.9):\n",
    "#     bins = [\n",
    "#         create_bins(num_bins, -1, 1),    # x position\n",
    "#         create_bins(num_bins, -1, 1),    # y position\n",
    "#         create_bins(num_bins, -1, 1),    # x velocity\n",
    "#         create_bins(num_bins, -1, 1),    # y velocity\n",
    "#         create_bins(num_bins, -1, 1),    # angle\n",
    "#         create_bins(num_bins, -1, 1),    # angular velocity\n",
    "#         create_bins(num_bins, 0, 1),     # left leg contact\n",
    "#         create_bins(num_bins, 0, 1)      # right leg contact\n",
    "#     ]\n",
    "#     agent = QLearningAgent(tuple([num_bins] * len(bins)), env.action_space, learning_rate, gamma)\n",
    "#     rewards = []\n",
    "\n",
    "#     for episode in range(num_episodes):\n",
    "#         state, _ = env.reset()\n",
    "#         state = digitize_state(state, bins)\n",
    "#         total_reward = 0\n",
    "#         done = False\n",
    "\n",
    "#         while not done:\n",
    "#             action = agent.choose_action(state)\n",
    "#             next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "#             done = terminated or truncated\n",
    "#             next_state = digitize_state(next_state, bins)\n",
    "#             total_reward += reward\n",
    "\n",
    "#             agent.update_q_value(state, action, reward, next_state)\n",
    "#             state = next_state\n",
    "#         rewards.append(total_reward)\n",
    "#         if (episode + 1) % 1000 == 0:\n",
    "#             print(f'Episode {episode + 1}, Total Reward: {total_reward}')\n",
    "\n",
    "#     return agent, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Initialize the environment\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "env.reset()\n",
    "\n",
    "# Initialize hyperparameters\n",
    "episodes = 50000\n",
    "alpha = 0.1\n",
    "gamma = 0.9\n",
    "epsilon = 0.1\n",
    "\n",
    "# Initialize array to store total reward for each episode\n",
    "rewards = np.zeros(episodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create bins for discretizing the state space\n",
    "def create_bins(num_bins, lower_bound, upper_bound):\n",
    "    return np.linspace(lower_bound, upper_bound, num_bins + 1)[1:-1]\n",
    "\n",
    "# Digitize the state\n",
    "def digitize_state(state, bins):\n",
    "    digitized_state = []\n",
    "    for i in range(len(state)):\n",
    "        digitized_state.append(np.digitize(state[i], bins[i]))\n",
    "    return tuple(digitized_state)\n",
    "\n",
    "# Define the number of bins\n",
    "num_bins = 4\n",
    "bins = [\n",
    "    create_bins(num_bins, -1, 1),  # x position\n",
    "    create_bins(num_bins, -1, 1),  # y position\n",
    "    create_bins(num_bins, -1, 1),  # x velocity\n",
    "    create_bins(num_bins, -1, 1),  # y velocity\n",
    "    create_bins(num_bins, -1, 1),  # angle\n",
    "    create_bins(num_bins, -1, 1),  # angular velocity\n",
    "    create_bins(num_bins, 0, 1),   # left leg contact\n",
    "    create_bins(num_bins, 0, 1)    # right leg contact\n",
    "]\n",
    "\n",
    "# Initialize Q-table\n",
    "num_columns = env.action_space.n\n",
    "qtable = np.zeros(tuple([num_bins] * len(bins)) + (num_columns,))\n",
    "\n",
    "# Function to run each episode\n",
    "def episode():\n",
    "    total_reward = 0\n",
    "    state = env.reset()[0]\n",
    "    state = digitize_state(state, bins)\n",
    "    done = False\n",
    "    g = 0\n",
    "    while not done:\n",
    "        # Choose action by epsilon-greedy policy\n",
    "        g += 1 \n",
    "        if np.random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        elif np.max(qtable[state]) > 0:\n",
    "            action = np.argmax(qtable[state])\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        new_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated  # Becomes true when episode terminates or gets truncated\n",
    "        total_reward += reward\n",
    "        new_state = digitize_state(new_state, bins)\n",
    "        \n",
    "        # Update Q-table\n",
    "        qtable[state + (action,)] += alpha * (reward + gamma * np.max(qtable[new_state]) - qtable[state + (action,)])\n",
    "\n",
    "        state = new_state\n",
    "        avg_rew = total_reward/g\n",
    "\n",
    "    return avg_rew\n",
    "\n",
    "# Run the episodes\n",
    "for epi in range(episodes):\n",
    "    rewards[epi] = episode()\n",
    "\n",
    "print(qtable)\n",
    "\n",
    "# Plot reward vs episode curves\n",
    "def plot_rewards(rewards, title):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(rewards)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.show()\n",
    "\n",
    "plot_rewards(rewards, 'Reward vs Episode (Discrete State, Q-Learning)')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_rewards(rewards, title):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(rewards)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.show()\n",
    "\n",
    "plot_rewards(rewards_cont, 'Reward vs Episode (Continuous State, Linear Value Function)')\n",
    "plot_rewards(rewards_disc, 'Reward vs Episode (Discrete State, Q-Learning)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def plot_heatmap(weights, title):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.heatmap(weights.reshape(1, -1), annot=True, fmt=\".2f\", cmap=\"viridis\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "plot_heatmap(value_function.weights, 'Linear Value Function Weights')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "class LinearQFunction:\n",
    "    def __init__(self, state_dim, action_dim, learning_rate=0.01):\n",
    "        self.weights = np.zeros((state_dim, action_dim))\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def predict(self, state):\n",
    "        return np.dot(state, self.weights)\n",
    "\n",
    "    def update(self, state, action, target):\n",
    "        prediction = self.predict(state)[action]\n",
    "        error = target - prediction\n",
    "        self.weights[:, action] += self.learning_rate * error * state\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, state_dim, action_dim, learning_rate=0.1, discount_factor=0.99, exploration_rate=1.0):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.q_function = LinearQFunction(state_dim, action_dim, learning_rate)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            return np.random.randint(self.action_dim)\n",
    "        return np.argmax(self.q_function.predict(state))\n",
    "\n",
    "    def update_q_value(self, state, action, reward, next_state, done):\n",
    "        best_next_action = np.argmax(self.q_function.predict(next_state))\n",
    "        td_target = reward + self.discount_factor * self.q_function.predict(next_state)[best_next_action] * (not done)\n",
    "        self.q_function.update(state, action, td_target)\n",
    "\n",
    "def q_learning(env, num_episodes=50000, learning_rate=0.1, gamma=0.9):\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    agent = QLearningAgent(state_dim, action_dim, learning_rate, gamma)\n",
    "    rewards = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = agent.choose_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            total_reward += reward\n",
    "\n",
    "            agent.update_q_value(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "        rewards.append(total_reward)\n",
    "        if (episode + 1) % 1000 == 0:\n",
    "            print(f'Episode {episode + 1}, Total Reward: {total_reward}')\n",
    "\n",
    "    return agent, rewards\n",
    "\n",
    "env_disc = gym.make(\"LunarLander-v2\")\n",
    "agent, rewards_disc = q_learning(env_disc)\n",
    "\n",
    "# Plot reward vs episode curves\n",
    "def plot_rewards(rewards, title):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(rewards)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.show()\n",
    "\n",
    "plot_rewards(rewards_disc, 'Reward vs Episode (Continuous State, Linear Q-learning)')\n",
    "\n",
    "# Heatmap of the Q-function weights for a sample action (e.g., action 0)\n",
    "def plot_heatmap(weights, action, title):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.heatmap(weights[:, action].reshape(1, -1), annot=True, fmt=\".2f\", cmap=\"viridis\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "plot_heatmap(agent.q_function.weights, 0, 'Linear Q-learning Weights for Action 0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
